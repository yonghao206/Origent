{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-Data-and-Packages\" data-toc-modified-id=\"Load-Data-and-Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load Data and Packages</a></span></li><li><span><a href=\"#Gluon\" data-toc-modified-id=\"Gluon-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Gluon</a></span></li><li><span><a href=\"#output-Train-and-Test-Files\" data-toc-modified-id=\"output-Train-and-Test-Files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>output Train and Test Files</a></span><ul class=\"toc-item\"><li><span><a href=\"#output-DNN-predicted-train-value\" data-toc-modified-id=\"output-DNN-predicted-train-value-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>output DNN predicted train value</a></span></li><li><span><a href=\"#output-DNN-predicted-test-data\" data-toc-modified-id=\"output-DNN-predicted-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>output DNN predicted test data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:42.302560Z",
     "start_time": "2019-05-04T17:16:40.159804Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:44.756512Z",
     "start_time": "2019-05-04T17:16:44.039215Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './output'\n",
    "df = pd.read_csv(path+'./data_processed_0411.csv',index_col=None)\n",
    "label = 'r_alsfrs_r_total'\n",
    "feature_selected = df.columns.tolist()\n",
    "feature_selected.remove('pid')\n",
    "feature_selected.remove(label)\n",
    "feature_selected.remove('set')\n",
    "df.loc[:,feature_selected] = df[feature_selected].apply(lambda x: (x-x.mean())/x.std())\n",
    "df_pro = df[df.set==1]\n",
    "df_test = df[df.set==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T04:12:23.944193Z",
     "start_time": "2019-04-14T04:12:23.940204Z"
    }
   },
   "source": [
    "# Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:46.384264Z",
     "start_time": "2019-05-04T17:16:46.354344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "square_loss = gluon.loss.L2Loss()\n",
    "def get_rmse(net, X_train,y_train):\n",
    "    clipped_preds = nd.clip(net(X_train), 1, 48)\n",
    "    return nd.sqrt(2*square_loss(clipped_preds, y_train).mean()).asscalar()\n",
    "\n",
    "# define network structure\n",
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(256, activation='relu'))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(128, activation='relu'))\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(64, activation='relu'))\n",
    "        net.add(gluon.nn.Dropout(0.3))\n",
    "        net.add(gluon.nn.Dense(32, activation='relu'))\n",
    "        net.add(gluon.nn.Dropout(0.1))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:47.259921Z",
     "start_time": "2019-05-04T17:16:47.232993Z"
    }
   },
   "outputs": [],
   "source": [
    "# used to split data based on patient id\n",
    "np.random.seed(10)\n",
    "pid_list = list(set(df_pro['pid']))\n",
    "mylist = []\n",
    "for k in range(0, (len(pid_list))):\n",
    "    x = np.random.randint(0, 5)\n",
    "    mylist.append(x)     \n",
    "columns = ['pid', 'cv_cohort']\n",
    "cohort = pd.DataFrame(columns=columns)\n",
    "cohort['pid'] = pid_list\n",
    "cohort['cv_cohort'] = mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:48.174475Z",
     "start_time": "2019-05-04T17:16:48.077735Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(k,net, X_train, y_train, X_test, y_test, df_test, epochs, verbose_epoch, learning_rate, weight_decay):\n",
    "    print('training....')\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    return_predicted_value = []\n",
    "    return_predicted_value_true = []\n",
    "    \n",
    "    batch_size = 16\n",
    "    # train data loaded\n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "    # parameter initialize firstly\n",
    "    net.collect_params().initialize(force_reinit=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cur_train_loss = get_rmse(net, X_train, y_train)\n",
    "\n",
    "        print(\"Epoch %d, train loss: %f\" % (epoch, cur_train_loss))        \n",
    "        cur_test_loss = get_rmse(net, X_test, y_test)\n",
    "        print(\"Epoch %d, test_loss loss: %f\" % (epoch, cur_test_loss))\n",
    "\n",
    "        train_loss.append(cur_train_loss)\n",
    "        if X_test is not None:\n",
    "            cur_test_loss = get_rmse(net, X_test, y_test)\n",
    "            test_loss.append(cur_test_loss)\n",
    "    return_predicted_value.extend(net(X_test))\n",
    "    return_predicted_value_true.extend(net(nd.array(df_test[feature_selected].values)))\n",
    "    plt.plot(train_loss)\n",
    "    plt.legend(['train'])\n",
    "    if X_test is not None:\n",
    "        plt.plot(test_loss)\n",
    "        plt.legend(['train','test'])\n",
    "    plt.savefig('./train_test_loss_'+str(k)+'.png')\n",
    "    plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss, return_predicted_value, return_predicted_value_true\n",
    "    else:\n",
    "        return cur_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:50.220003Z",
     "start_time": "2019-05-04T17:16:50.155178Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_valid(k, epochs, verbose_epoch, df_pro, df_test, learning_rate, weight_decay, feature_selected, label, cohort):\n",
    "    import time\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    temp_test =pd.DataFrame()\n",
    "    temp_test_real =  []\n",
    "    predicted_value = []\n",
    "    true_test_value = []\n",
    "    for i in range(k):\n",
    "        a = time.time()\n",
    "        train_pid = cohort['pid'][cohort['cv_cohort'] != i]\n",
    "        test_pid = cohort['pid'][cohort['cv_cohort'] == i]\n",
    "        train = df_pro[df_pro['pid'].isin(train_pid)]\n",
    "        test = df_pro[df_pro['pid'].isin(test_pid)]\n",
    "\n",
    "        train_x = nd.array(train[feature_selected][:])\n",
    "        train_y = nd.array(train[label][:])\n",
    "\n",
    "        test_x = nd.array(test[feature_selected][:])\n",
    "        test_y = nd.array(test[label][:])\n",
    "        temp_test = pd.concat([temp_test, test[['pid', 't', 'month', label]]])\n",
    "    \n",
    "        net = get_net()\n",
    "        train_loss, test_loss, return_predicted_value, return_predicted_value_true = train_model(i,net, train_x, train_y, test_x, test_y, df_test, epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        predicted_value.extend(return_predicted_value)\n",
    "        temp_test_real.append(return_predicted_value_true)\n",
    "        true_test_value.extend(test_y)\n",
    "        \n",
    "        train_loss_sum += train_loss\n",
    "        print(\"Test loss: %f\" % test_loss)\n",
    "        print('time pass per fold: ', time.time()-a)\n",
    "        test_loss_sum += test_loss\n",
    "        \n",
    "    return train_loss_sum / k, test_loss_sum / k, predicted_value, temp_test, temp_test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:50.396531Z",
     "start_time": "2019-05-04T17:16:50.391546Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# huper-parameters\n",
    "weight_decay = 0.05 #5 10 20\n",
    "k = 5\n",
    "epochs =25 #2 20 40\n",
    "verbose_epoch = 10 \n",
    "learning_rate = 0.0001 #0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:53.799082Z",
     "start_time": "2019-05-04T17:16:53.795093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, test_loss, predicted_train, temp_test, temp_test_real = k_fold_cross_valid(k, epochs, verbose_epoch, df_pro, df_test, learning_rate, weight_decay, feature_selected, label, cohort)\n",
    "print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" % (k, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output Train and Test Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T13:13:15.263146Z",
     "start_time": "2019-04-15T13:13:14.205975Z"
    }
   },
   "source": [
    "## output DNN predicted train value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:16:58.191225Z",
     "start_time": "2019-05-04T17:16:58.187236Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_tra = [predicted_train[i].asnumpy()[0] for i in range(len(predicted_train))]\n",
    "temp_test['test_value'] = predicted_tra\n",
    "temp_test.to_csv('./output/train_predicted_value_DNN.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T14:24:27.169611Z",
     "start_time": "2019-04-15T14:24:27.045944Z"
    }
   },
   "source": [
    "## output DNN predicted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T17:17:01.960708Z",
     "start_time": "2019-05-04T17:17:01.955721Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predicted_all = []\n",
    "for j in range(len(temp_test_real)):\n",
    "    test_predicted_all.append([temp_test_real[j][i].asnumpy()[0] for i in range(len(temp_test_real[j]))])\n",
    "    \n",
    "test_table = df_test[['pid', 't', 'month', label]].copy()\n",
    "test_table.reset_index(drop=True, inplace=True)\n",
    "test_table.rename(columns={'r_alsfrs_r_total':'true'},inplace=True)\n",
    "test_table['mod3_DNN'] = np.array(test_predicted_all).mean(axis=0)\n",
    "test_table.to_csv('./output/test_predicted_value_DNN.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 364.090454,
   "position": {
    "height": "385.534px",
    "left": "370.364px",
    "right": "20px",
    "top": "95px",
    "width": "699.523px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
